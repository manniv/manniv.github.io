<!DOCTYPE html><html lang="en"><head><meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="assets/w_page.css">

<script src="assets/d3.min.js"></script>
<script src="assets/d3-path.min.js"></script>
<script src="assets/underscore.js"></script>
<script src="assets/vis_functions.js"></script>

<script type="text/front-matter">
  title: Deconvolution and Checkerboard Artifacts
  description: When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.
  authors:
  - Augustus Odena
  - Vincent Dumoulin: http://vdumoulin.github.io/
  - Chris Olah: http://colah.github.io
  affiliations:
  - Google Brain: http://g.co/brain
  - Université de Montréal: https://mila.umontreal.ca/en/
  - Google Brain: http://g.co/brain
</script>

<meta name="viewport" content="width=device-width, initial-scale=1"><style>html {
  font: 400 16px/1.55em -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
  /*background-color: hsl(223, 9%, 25%);*/
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
}
body {
  margin: 0;
  /*background-color: hsl(223, 9%, 25%);*/
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

h1 {
  font-family: Cochin, Georgia, serif;
}

/*
html, body, div, span, applet, object, iframe,
h1, h2, h3, h4, h5, h6, p, blockquote, pre,
a, abbr, acronym, address, big, cite, code,
del, dfn, em, img, ins, kbd, q, s, samp,
small, strike, strong, sub, sup, tt, var,
b, u, i, center,
dl, dt, dd, ol, ul, li,
fieldset, form, label, legend,
table, caption, tbody, tfoot, thead, tr, th, td,
article, aside, canvas, details, embed,
figure, figcaption, footer, header, hgroup,
menu, nav, output, ruby, section, summary,
time, mark, audio, video {
	margin: 0;
	padding: 0;
	border: 0;
	font-size: 100%;
	font: inherit;
	vertical-align: baseline;
}
article, aside, details, figcaption, figure,
footer, header, hgroup, menu, nav, section {
	display: block;
}
body {
	line-height: 1;
}
ol, ul {
	list-style: none;
}
blockquote, q {
	quotes: none;
}
blockquote:before, blockquote:after,
q:before, q:after {
	content: '';
	content: none;
}
table {
	border-collapse: collapse;
	border-spacing: 0;
}*/
/*
  Column: 60px
  Gutter: 24px

  Body: 648px
    - 8 columns
    - 7 gutters
  Middle: 816px
  Page: 984px
    - 12 columns
    - 11 gutters
*/

.l-body,
.l-body-outset,
.l-page,
.l-page-outset,
.l-middle,
.l-middle-outset,
dt-article > div,
dt-article > p,
dt-article > h1,
dt-article > h2,
dt-article > h3,
dt-article > h4,
dt-article > figure,
dt-article > table,
dt-article > ol,
dt-article > ul,
dt-article > dt-byline,
dt-article > dt-math,
dt-article > dt-code,
dt-article section > div,
dt-article section > p,
dt-article section > h1,
dt-article section > h2,
dt-article section > h3,
dt-article section > h4,
dt-article section > figure,
dt-article section > table,
dt-article section > ol,
dt-article section > ul,
dt-article section > dt-byline,
dt-article section > dt-code {
  width: auto;
  margin-left: 24px;
  margin-right: 24px;
  box-sizing: border-box;
}

@media(min-width: 768px) {
  .l-body,
  .l-body-outset,
  .l-page,
  .l-page-outset,
  .l-middle,
  .l-middle-outset,
  dt-article > div,
  dt-article > p,
  dt-article > h1,
  dt-article > h2,
  dt-article > h3,
  dt-article > h4,
  dt-article > figure,
  dt-article > table,
  dt-article > ol,
  dt-article > ul,
  dt-article > dt-byline,
  dt-article > dt-math,
  dt-article > dt-code,
  dt-article section > div,
  dt-article section > p,
  dt-article section > h1,
  dt-article section > h2,
  dt-article section > h3,
  dt-article section > h4,
  dt-article section > figure,
  dt-article section > table,
  dt-article section > ol,
  dt-article section > ul,
  dt-article section > dt-byline,
  dt-article section > dt-code {
    margin-left: 72px;
    margin-right: 72px;
  }
}

@media(min-width: 1080px) {
  .l-body,
  dt-article > div,
  dt-article > p,
  dt-article > h2,
  dt-article > h3,
  dt-article > h4,
  dt-article > figure,
  dt-article > table,
  dt-article > ol,
  dt-article > ul,
  dt-article > dt-byline,
  dt-article > dt-math,
  dt-article > dt-code,
  dt-article section > div,
  dt-article section > p,
  dt-article section > h2,
  dt-article section > h3,
  dt-article section > h4,
  dt-article section > figure,
  dt-article section > table,
  dt-article section > ol,
  dt-article section > ul,
  dt-article section > dt-byline,
  dt-article section > dt-code {
    margin-left: calc(50% - 984px / 2);
    width: 648px;
  }
  .l-body-outset,
  dt-article .l-body-outset {
    margin-left: calc(50% - 984px / 2 - 96px/2);
    width: calc(648px + 96px);
  }
  .l-middle,
  dt-article .l-middle {
    width: 816px;
    margin-left: calc(50% - 984px / 2);
    margin-right: auto;
  }
  .l-middle-outset,
  dt-article .l-middle-outset {
    width: calc(816px + 96px);
    margin-left: calc(50% - 984px / 2 - 48px);
    margin-right: auto;
  }
  dt-article > h1,
  dt-article section > h1,
  .l-page,
  dt-article .l-page,
  dt-article.centered .l-page {
    width: 984px;
    margin-left: auto;
    margin-right: auto;
  }
  .l-page-outset,
  dt-article .l-page-outset,
  dt-article.centered .l-page-outset {
    width: 1080px;
    margin-left: auto;
    margin-right: auto;
  }
  .l-screen,
  dt-article .l-screen,
  dt-article.centered .l-screen {
    margin-left: auto;
    margin-right: auto;
    width: auto;
  }
  .l-screen-inset,
  dt-article .l-screen-inset,
  dt-article.centered .l-screen-inset {
    margin-left: 24px;
    margin-right: 24px;
    width: auto;
  }
  .l-gutter,
  dt-article .l-gutter {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 24px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
    width: calc((984px - 648px) / 2 - 24px);
  }

  /* Side */
  .side.l-body,
  dt-article .side.l-body {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 984px + 648px) / 2);
    width: calc(648px / 2 - 24px - 84px);
  }
  .side.l-body-outset,
  dt-article .side.l-body-outset {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 984px + 648px - 48px) / 2);
    width: calc(648px / 2 - 48px + 24px);
  }
  .side.l-middle,
  dt-article .side.l-middle {
    clear: both;
    float: right;
    width: calc(456px - 84px);
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .side.l-middle-outset,
  dt-article .side.l-middle-outset {
    clear: both;
    float: right;
    width: 456px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .side.l-page,
  dt-article .side.l-page {
    clear: both;
    float: right;
    margin-left: 48px;
    width: calc(624px - 84px);
    margin-right: calc((100vw - 984px) / 2);
  }
  .side.l-page-outset,
  dt-article .side.l-page-outset {
    clear: both;
    float: right;
    width: 624px;
    margin-right: calc((100vw - 984px) / 2);
  }
}

/* Centered */

@media(min-width: 1080px) {
  .centered .l-body,
  .centered.l-body,
  dt-article.centered > div,
  dt-article.centered > p,
  dt-article.centered > h2,
  dt-article.centered > h3,
  dt-article.centered > h4,
  dt-article.centered > figure,
  dt-article.centered > table,
  dt-article.centered > ol,
  dt-article.centered > ul,
  dt-article.centered > dt-byline,
  dt-article.centered > dt-code,
  dt-article.centered section > div,
  dt-article.centered section > p,
  dt-article.centered section > h2,
  dt-article.centered section > h3,
  dt-article.centered section > h4,
  dt-article.centered section > figure,
  dt-article.centered section > table,
  dt-article.cebtered section > ol,
  dt-article.centered section > ul,
  dt-article.centered section > dt-byline,
  dt-article.centered section > dt-code,
  dt-article section.centered > div,
  dt-article section.centered > p,
  dt-article section.centered > h2,
  dt-article section.centered > h3,
  dt-article section.centered > h4,
  dt-article section.centered > figure,
  dt-article section.centered > table,
  dt-article section.centered > ol,
  dt-article section.centered > ul,
  dt-article section.centered > dt-byline,
  dt-article section.centered > dt-code {
    margin-left: auto;
    margin-right: auto;
    width: 648px;
  }
  .centered .l-body-outset,
  .centered.l-body-outset,
  dt-article.centered .l-body-outset {
    margin-left: auto;
    margin-right: auto;
    width: calc(648px + 96px);
  }
  dt-article.centered > h1,
  dt-article.centered section > h1,
  dt-article section.centered > h1,
  .centered .l-middle,
  .centered.l-middle,
  dt-article.centered .l-middle {
    width: 816px;
    margin-left: auto;
    margin-right: auto;
  }

  .centered .l-middle-outset,
  .centered.l-middle-outset,
  dt-article.centered .l-middle-outset {
    width: calc(816px + 96px);
    margin-left: auto;
    margin-right: auto;
  }

  /* page and screen are already centered */

  /* Side */

  .centered .side.l-body,
  .centered dt-article .side.l-body {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 648px) / 2);
    width: calc(4 * 60px + 3 * 24px);
  }
  .centered .side.l-body-outset,
  .centered dt-article .side.l-body-outset {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 48px;
    margin-right: calc((100vw - 648px) / 2);
    width: calc(4 * 60px + 3 * 24px);
  }
  .centered .side.l-middle,
  .centered dt-article .side.l-middle {
    clear: both;
    float: right;
    width: 396px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px / 2);
  }
  .centered .side.l-middle-outset,
  .centered dt-article .side.l-middle-outset {
    clear: both;
    float: right;
    width: 456px;
    margin-left: 48px;
    margin-right: calc((100vw - 984px) / 2 + 168px);
  }
  .centered .side.l-page,
  .centered dt-article .side.l-page {
    clear: both;
    float: right;
    width: 480px;
    margin-right: calc((100vw - 984px) / 2);
  }
  .centered .side.l-page-outset,
  .centered dt-article .side.l-page-outset {
    clear: both;
    float: right;
    width: 480px;
    margin-right: calc((100vw - 984px) / 2);
  }
  .centered .l-gutter,
  .centered.l-gutter,
  dt-article.centered .l-gutter {
    clear: both;
    float: right;
    margin-top: 0;
    margin-left: 24px;
    margin-right: calc((100vw - 984px) / 2);
    width: calc((984px - 648px) / 2 - 24px);
  }

}

/* Rows and Columns */

.row {
  display: flex;
}
.column {
  flex: 1;
  box-sizing: border-box;
  margin-right: 24px;
  margin-left: 24px;
}
.row > .column:first-of-type {
  margin-left: 0;
}
.row > .column:last-of-type {
  margin-right: 0;
}
dt-article {
  display: block;
  color: rgba(0, 0, 0, 0.8);
  font: 17px/1.55em -apple-system, BlinkMacSystemFont, "Roboto", sans-serif;
  padding-bottom: 72px;
  background: white;
}

@media(min-width: 1024px) {
  dt-article {
    font-size: 20px;
  }
}

/* H1 */

dt-article h1 {
  margin-top: 18px;
  font-weight: 400;
  font-size: 40px;
  line-height: 1em;
  font-family: HoeflerText-Regular, Cochin, Georgia, serif;
}
@media(min-width: 768px) {
  dt-article h1 {
    font-size: 46px;
    margin-top: 48px;
    margin-bottom: 12px;
  }
}

@media(min-width: 1080px) {
  .centered h1 {
    text-align: center;
  }

  dt-article h1 {
    font-size: 50px;
    letter-spacing: -0.02em;
  }

  dt-article > h1:first-of-type,
  dt-article section > h1:first-of-type {
    margin-top: 80px;
  }
}


@media(min-width: 1200px) {
  dt-article h1 {
    font-size: 56px;
  }

  dt-article > h1:first-of-type {
    margin-top: 100px;
  }
}

/* H2 */

dt-article h2 {
  font-family: HoeflerText-Regular, Cochin, Georgia, serif;
  font-weight: 400;
  font-size: 26px;
  line-height: 1.25em;
  margin-top: 36px;
  margin-bottom: 24px;
}

@media(min-width: 1024px) {
  dt-article h2 {
    margin-top: 48px;
    font-size: 30px;
  }
}

dt-article h1 + h2 {
  font-weight: 300;
  font-size: 20px;
  line-height: 1.4em;
  margin-top: 8px;
  font-style: normal;
}


@media(min-width: 1080px) {
  .centered h1 + h2 {
    text-align: center;
  }
  dt-article h1 + h2 {
    margin-top: 12px;
    font-size: 24px;
  }
}

/* H3 */

dt-article h3 {
  font-family: HoeflerText-Regular, Georgia, serif;
  font-weight: 400;
  font-size: 20px;
  line-height: 1.4em;
  margin-top: 36px;
  margin-bottom: 18px;
  font-style: italic;
}

dt-article h1 + h3 {
  margin-top: 48px;
}

@media(min-width: 1024px) {
  dt-article h3 {
    font-size: 26px;
  }
}

/* H4 */

dt-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

dt-article a {
  color: inherit;
}

dt-article p,
dt-article ul,
dt-article ol {
  margin-bottom: 24px;
  font-family: Georgia, serif;
}

dt-article p b,
dt-article ul b,
dt-article ol b {
  -webkit-font-smoothing: antialiased;
}

dt-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

dt-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

dt-article .link {
  text-decoration: underline;
  cursor: pointer;
}

dt-article ul,
dt-article ol {
  padding-left: 24px;
}

dt-article li {
  margin-bottom: 24px;
  margin-left: 0;
  padding-left: 0;
}

dt-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}


dt-article hr {
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  margin-top: 60px;
  margin-bottom: 60px;
}

dt-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

/* Tables */

dt-article table {
  border-collapse: collapse;
}

dt-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

dt-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

dt-article table th,
dt-article table td {
  font-size: 15px;
  padding: 2px 0;
}

/* Figure */

dt-article figure {
  position: relative;
  margin-top: 30px;
  margin-bottom: 30px;
}

@media(min-width: 1024px) {
  dt-article figure {
    margin-top: 48px;
    margin-bottom: 48px;
  }
}

dt-article figure img {
  width: 100%;
}

dt-article figure svg text,
dt-article figure svg tspan {
}

dt-article figure figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}
@media(min-width: 1024px) {
  dt-article figure figcaption {
    font-size: 13px;
  }
}

dt-article figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

dt-article figure figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

dt-article figure figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

dt-article > dt-code,
dt-article section > dt-code  {
  display: block;
}

dt-article .citation {
  color: #668;
  cursor: pointer;
}

dt-include {
  width: auto;
  display: block;
}
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code {
  white-space: nowrap;
  background: rgba(0, 0, 0, 0.04);
  border-radius: 2px;
  padding: 4px 7px;
  font-size: 15px;
  color: rgba(0, 0, 0, 0.6);
}

pre code {
  display: block;
  background: white;
  border-left: 3px solid rgba(0, 0, 0, 0.05);
  padding: 0 0 0 24px;
}


code[class*="language-"],
pre[class*="language-"] {
  text-shadow: 0 1px white;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
  text-shadow: none;
  background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
  text-shadow: none;
  background: #b3d4fc;
}

@media print {
  code[class*="language-"],
  pre[class*="language-"] {
  text-shadow: none;
  }
}

/* Code blocks */
pre[class*="language-"] {
  overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: slategray;
}

.token.punctuation {
  color: #999;
}

.namespace {
  opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #a67f59;
  background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #07a;
}

.token.function {
  color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
  color: #e90;
}

.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}

.token.entity {
  cursor: help;
}

@media print {
  @page {
    size: 8in 11in;
  }
  html {
  }
  p, code {
    page-break-inside: avoid;
  }
  h2, h3 {
    page-break-after: avoid;
  }
  dt-header {
    visibility: hidden;
  }
  dt-footer {
    display: none!important;
  }
}
</style>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=
">
    <link href="/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
    <link rel="canonical" href="http://distill.pub/2016/deconv-checkerboard">
    <title>Deconvolution and Checkerboard Artifacts</title>
  
    <!--  https://schema.org/Article -->
    <meta property="article:published" itemprop="datePublished" content="2016-10-17">
    <meta property="article:created" itemprop="dateCreated" content="Mon Oct 17 2016 20:00:00 GMT+0000 (UTC)">
    <meta property="article:modified" itemprop="dateModified" content="Mon Aug 14 2017 20:52:14 GMT+0000 (UTC)">
  
      <meta property="article:author" content="Augustus Odena">
      <meta property="article:author" content="Vincent Dumoulin">
      <meta property="article:author" content="Chris Olah">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Deconvolution and Checkerboard Artifacts">
    <meta property="og:description" content="When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.">
    <meta property="og:url" content="http://distill.pub/2016/deconv-checkerboard">
    <meta property="og:image" content="http://distill.pub/2016/deconv-checkerboard/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Deconvolution and Checkerboard Artifacts">
    <meta name="twitter:description" content="When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.">
    <meta name="twitter:url" content="http://distill.pub/2016/deconv-checkerboard">
    <meta name="twitter:image" content="http://distill.pub/2016/deconv-checkerboard/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="Deconvolution and Checkerboard Artifacts">
    <meta name="citation_volume" content="1">
    <meta name="citation_issue" content="10">
    <meta name="citation_firstpage" content="e3">
    <meta name="citation_doi" content="10.23915/distill.00003">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2016/10/17">
    <meta name="citation_publication_date" content="2016/10/17">
    <meta name="citation_author" content="Odena, Augustus">
    <meta name="citation_author_institution" content="Google Brain">
    <meta name="citation_author" content="Dumoulin, Vincent">
    <meta name="citation_author_institution" content="Université de Montréal">
    <meta name="citation_author" content="Olah, Chris">
    <meta name="citation_author_institution" content="Google Brain">
    <meta name="citation_reference" content="citation_title=Unsupervised representation learning with deep convolutional generative adversarial networks;citation_author=Alec Radford;citation_author=Luke Metz;citation_author=Soumith Chintala;citation_publication_date=2015;citation_arxiv_id=1511.06434;">
    <meta name="citation_reference" content="citation_title=Improved techniques for training gans;citation_author=Tim Salimans;citation_author=Ian Goodfellow;citation_author=Wojciech Zaremba;citation_author=Vicki Cheung;citation_author=Alec Radford;citation_author=Xi Chen;citation_publication_date=2016;citation_arxiv_id=1606.03498;">
    <meta name="citation_reference" content="citation_title=Adversarial Feature Learning;citation_author=Jeff Donahue;citation_author=Philipp Krahenbuhl;citation_author=Trevor Darrell;citation_publication_date=2016;citation_arxiv_id=1605.09782;">
    <meta name="citation_reference" content="citation_title=Adversarially Learned Inference;citation_author=Vincent Dumoulin;citation_author=Ishmael Belghazi;citation_author=Ben Poole;citation_author=Alex Lamb;citation_author=Martin Arjovsky;citation_author=Olivier Mastropietro;citation_author=Aaron Courville;citation_publication_date=2016;citation_arxiv_id=1606.00704;">
    <meta name="citation_reference" content="citation_title=A guide to convolution arithmetic for deep learning;citation_author=Vincent Dumoulin;citation_author=Francesco Visin;citation_publication_date=2016;citation_arxiv_id=1603.07285;">
    <meta name="citation_reference" content="citation_title=Is the deconvolution layer the same as a convolutional layer?;citation_author=Wenzhe Shi;citation_author=Jose Caballero;citation_author=Lucas Theis;citation_author=Ferenc Huszar;citation_author=Andrew Aitken;citation_author=Christian Ledig;citation_author=Zehan Wang;citation_publication_date=2016;citation_arxiv_id=1609.07009;">
    <meta name="citation_reference" content="citation_title=Conditional generative adversarial nets for convolutional face generation;citation_author=Jon Gauthier;citation_publication_date=2014;citation_journal_title=Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester;citation_volume=2014;">
    <meta name="citation_reference" content="citation_title=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network;citation_author=Wenzhe Shi;citation_author=Jose Caballero;citation_author=Ferenc Huszar;citation_author=Johannes Totz;citation_author=Andrew P Aitken;citation_author=Rob Bishop;citation_author=Daniel Rueckert;citation_author=Zehan Wang;citation_publication_date=2016;citation_arxiv_id=1609.05158;">
    <meta name="citation_reference" content="citation_title=Image super-resolution using deep convolutional networks;citation_author=Chao Dong;citation_author=Chen Change Loy;citation_author=Kaiming He;citation_author=Xiaoou Tang;citation_publication_date=2014;citation_arxiv_id=1501.00092;">
    <meta name="citation_reference" content="citation_title=Perceptual losses for real-time style transfer and super-resolution;citation_author=Justin Johnson;citation_author=Alexandre Alahi;citation_author=Li Fei-Fei;citation_publication_date=2016;citation_arxiv_id=1603.08155;">
    <meta name="citation_reference" content="citation_title=Inceptionism: Going deeper into neural networks;citation_author=Alexander Mordvintsev;citation_author=Christopher Olah;citation_author=Mike Tyka;citation_publication_date=2015;citation_journal_title=Google Research Blog. Retrieved June;citation_volume=20;">
    <meta name="citation_reference" content="citation_title=Geodesics of learned representations;citation_author=Olivier J Henaff;citation_author=Eero P Simoncelli;citation_publication_date=2015;citation_arxiv_id=1511.06394;">
    <meta name="citation_reference" content="citation_title=DeepDreaming with TensorFlow;citation_author=Alexander Mordvintsev;citation_publication_date=2016;">
</head><body><dt-header>
<style>
dt-header {
  display: block;
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
dt-header .content {
  height: 70px;
}
dt-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
dt-header a:hover {
  color: rgba(255, 255, 255, 1);
}
dt-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  dt-header {
    height: 70px;
  }
  dt-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  dt-header .logo {
  }
}
dt-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
dt-header .logo {
  font-size: 17px;
  font-weight: 200;
}
dt-header .nav {
  float: right;
  font-weight: 300;
}
dt-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>

<div class="content l-page">
  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  <div class="nav">
    <a href="/about/">About</a>
    <a href="/prize/">Prize</a>
    <a href="/journal/">Submit</a>
  </div>
</div>
</dt-header><dt-article class="centered">
  <h1>Deconvolution and Checkerboard Artifacts</h1>

  <dt-byline>
<style>
  dt-byline {
    font-size: 12px;
    line-height: 18px;
    display: block;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0, 0, 0, 0.5);
    padding-top: 12px;
    padding-bottom: 12px;
  }
  dt-article.centered dt-byline {
    text-align: center;

  }
  dt-byline a,
  dt-article dt-byline a {
    text-decoration: none;
    border-bottom: none;
  }
  dt-article dt-byline a:hover {
    text-decoration: underline;
    border-bottom: none;
  }
  dt-byline .authors {
    text-align: left;
  }
  dt-byline .name {
    display: inline;
    text-transform: uppercase;
  }
  dt-byline .affiliation {
    display: inline;
  }
  dt-byline .date {
    display: block;
    text-align: left;
  }
  dt-byline .year, dt-byline .month {
    display: inline;
  }
  dt-byline .citation {
    display: block;
    text-align: left;
  }
  dt-byline .citation div {
    display: inline;
  }

  @media(min-width: 768px) {
    dt-byline {
    }
  }

  @media(min-width: 1080px) {
    dt-byline {
      border-bottom: none;
      margin-bottom: 70px;
    }

    dt-byline a:hover {
      color: rgba(0, 0, 0, 0.9);
    }

    dt-byline .authors {
      display: inline-block;
    }

    dt-byline .author {
      display: inline-block;
      margin-right: 12px;
      /*padding-left: 20px;*/
      /*border-left: 1px solid #ddd;*/
    }

    dt-byline .affiliation {
      display: block;
    }

    dt-byline .author:last-child {
      margin-right: 0;
    }

    dt-byline .name {
      display: block;
    }

    dt-byline .date {
      border-left: 1px solid rgba(0, 0, 0, 0.1);
      padding-left: 15px;
      margin-left: 15px;
      display: inline-block;
    }
    dt-byline .year, dt-byline .month {
      display: block;
    }

    dt-byline .citation {
      border-left: 1px solid rgba(0, 0, 0, 0.15);
      padding-left: 15px;
      margin-left: 15px;
      display: inline-block;
    }
    dt-byline .citation div {
      display: block;
    }
  }
</style>


<div class="byline">
  <div class="authors">
    <div class="author">
        <div class="name">Augustus Odena</div>
          <a class="affiliation" href="http://g.co/brain">Google Brain</a>
    </div>
    <div class="author">
        <a class="name" href="http://vdumoulin.github.io/">Vincent Dumoulin</a>
          <a class="affiliation" href="https://mila.umontreal.ca/en/">Université de Montréal</a>
    </div>
    <div class="author">
        <a class="name" href="http://colah.github.io">Chris Olah</a>
          <a class="affiliation" href="http://g.co/brain">Google Brain</a>
    </div>
  </div>
  <div class="date">
    <div class="month">Oct. 17</div>
    <div class="year">2016</div>
  </div>
  <a class="citation" href="#citation">
    <div>Citation:</div>
    <div>Odena, et al., 2016</div>
  </a>
</div>
</dt-byline>

  <p>When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. It’s more obvious in some cases than others, but a large fraction of recent models exhibit this behavior.</p>

  <dt-include src="assets/samples.html" class="l-middle"><style>
.pattern-examples {
  width: 100%;
}
.pattern-examples > figure {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  position: relative;
}

.pattern-examples > figure:after {
  overflow: visible;
  content: "";
  background-image: url("assets/pointer.svg");
  width: 27px;
  height: 27px;
  position: absolute;
  right: -51px;
}

.pattern-examples .example {
  width: 100%;
  margin-right: 2.5%;
}

.pattern-examples .example:last-of-type {
  margin-right: 0;
}

.pattern-examples .original {
  position: relative;
}

.pattern-examples .original:after {
  content: "";
  display: block;
  width: 100%;
  height: 150%;
  position: absolute;
  top: -20%;
  z-index: 1;
}

.pattern-examples .example .reticle {
  position: absolute;
  pointer-events: none;
}

.pattern-examples .example .reticle:after {
  box-sizing: border-box;
  position: relative;
  width: 100%;
  height: 100%;
  left: -50%;
  top: -50%;
  border-radius: 50%;
  border: 1px solid white;
  display: block;
  content: '';
  background-color: rgba(255, 255, 255, 0.2);
  box-shadow: 0 0 4px rgba(0, 0, 0, 0.5);
}

.pattern-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

.pattern-examples .closeup {
  display: block;
  width: 90%;
  border-radius: 50%;
  margin-bottom: 12px;
  position: relative;
  left: 5%;
  top: -5px;
  border: 2px solid white;
  overflow: hidden;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
}

.pattern-examples figcaption {
  width: 90%;
  margin-left: 5%;
  text-align: center;
}

.pattern-examples .closeup img {
  position: absolute;
  width: 800%;
}

.pattern-examples .closeup:after {
  padding-top: 100%;
  display: block;
  content: '';
}

</style>

<div class="pattern-examples">
  <figure class="">
    <div class="example">
      <div class="original" data-focus="5,0.87,0.24">
        <img src="assets/im_sample_dcgan.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/im_sample_dcgan_contrast.png">
      </div>
      <figcaption>
        <dt-cite key="radford2015unsupervised"><span id="citation-0" data-hover-ref="dt-cite-hover-box-0">Radford, et al., 2015 <span class="citation-number">[1]</span></span></dt-cite>
      </figcaption>
    </div>
    <div class="example">
      <div class="original" data-focus="10,0.05,0.84">
        <img src="assets/im_sample_salimans.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/im_sample_salimans.png">
      </div>
      <figcaption>
        <dt-cite key="salimans2016improved"><span id="citation-1" data-hover-ref="dt-cite-hover-box-1">Salimans et al., 2016 <span class="citation-number">[2]</span></span></dt-cite>
      </figcaption>
    </div>
    <div class="example">
      <div class="original" data-focus="5,0.70,0.28">
        <img src="assets/im_sample_donahue.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/im_sample_donahue.png">
      </div>
      <figcaption>
        <dt-cite key="donahue2016adversarial"><span id="citation-2" data-hover-ref="dt-cite-hover-box-2">Donahue, et al., 2016 <span class="citation-number">[3]</span></span></dt-cite>
      </figcaption>
    </div>
    <div class="example">
      <div class="original" data-focus="5,0.87,0.53">
        <img src="assets/im_sample_dumoulin.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/im_sample_dumoulin.png">
      </div>
      <figcaption>
        <dt-cite key="dumoulin2016adversarially"><span id="citation-3" data-hover-ref="dt-cite-hover-box-3">Dumoulin, et al., 2016 <span class="citation-number">[4]</span></span></dt-cite>
        <!--<b><i>Using our method.</i></b>-->
      </figcaption>
    </div>
  </figure>
</div>

<script>(function() {

var html = d3.select(".pattern-examples");
var original = html.selectAll(".example .original");
html
    .on("mouseleave", () => {
      console.log("out")
      original.each(resetReticle);
    });

original
    .datum(function() {
      var focus = this.getAttribute("data-focus").split(",")
      return {
        zoom: focus[0],
        x: focus[1],
        y: focus[2]
      };
    })
    .on("mouseleave", resetReticle)
    .on("mousemove", updateReticle)
    .each(resetReticle);

function updateReticle(d) {

  original.each(resetReticle);
  var x = d3.event.offsetX / this.getBoundingClientRect().width;
  var y = d3.event.offsetY / this.getBoundingClientRect().height;
  setPosition(this, x, y, d.zoom, 100);
}

function resetReticle(d) {
  setPosition(this, d.x, d.y, d.zoom, 300);
}

function setPosition(element, x, y, zoom, duration) {
  var margin = 1 / zoom / 2;
  x = Math.min(Math.max(margin, x), 1 - margin)
  y = Math.min(Math.max(margin, y), 1 - margin)
  var parent = d3.select(element.parentElement);
  var reticle = parent.select(".reticle");
  var closeup = parent.select(".closeup img");

  console.log(x, y)

  reticle
      .style("width", 100 / zoom + "%")
      .style("height", 100 / zoom + "%")
      .transition()
      .ease(d3.ease("cubic-out"))
      .duration(duration)
      .styleTween("left", function (d, i, a) {
        var from = this.style.left,
            to = x * 100 + "%";
        return d3.interpolateString(from, to);
      })
      .styleTween("top", function (d, i, a) {
        var from = this.style.top,
            to = y * 100 + "%";
        return d3.interpolateString(from, to);
      })

  closeup
      .style("width", zoom * 100 + "%")
      .style("height", zoom * 100 + "%")
      .transition()
      .ease(d3.ease("cubic-out"))
      .duration(duration)
      .styleTween("left", function (d, i, a) {
        var from = this.style.left,
            to = -x * 100 * zoom + 50 + "%";
        return d3.interpolateString(from, to);
      })
      .styleTween("top", function (d, i, a) {
        var from = this.style.top,
            to = -y * 100 * zoom + 50 + "%";
        return d3.interpolateString(from, to);
      })
}


})();</script>
</dt-include>

  <p>Mysteriously, the checkerboard pattern tends to be most prominent in images with strong colors.
  What’s going on? Do neural networks hate bright colors? The actual cause of these artifacts is actually remarkably simple, as is a method for avoiding them.</p>

  <hr>

  <h2>Deconvolution &amp; Overlap</h2>

  <p>When we have neural networks generate images, we often have them build them up
  from low resolution, high-level descriptions.
  This allows the network to describe the rough image and then fill in the details.</p>

  <p>In order to do this, we need some way to go from a lower resolution image to a higher one.
  We generally do this with the <em>deconvolution</em> operation.
  Roughly, deconvolution layers allow the model to use every point
  in the small image to “paint” a square in the larger one.</p>

  <p>(Deconvolution has a number of interpretations and different names, including “transposed convolution.”
  We use the name “deconvolution” in this article for brevity.
  For excellent discussion of deconvolution, see <dt-cite key="dumoulin2016guide,shi2016deconvolution"><span id="citation-4" data-hover-ref="dt-cite-hover-box-4"><span class="citation-number">[5, 6]</span></span></dt-cite>.)</p>

  <p>Unfortunately, deconvolution can easily have “uneven overlap,”
  putting more of the metaphorical paint in some places than others <dt-cite key="gauthier2014conditional"><span id="citation-5" data-hover-ref="dt-cite-hover-box-5"><span class="citation-number">[7]</span></span></dt-cite>.
  In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top).
  While the network could, in principle, carefully learn weights to avoid this
  — as we’ll discuss in more detail later —
  in practice neural networks struggle to avoid it completely.</p>

  <dt-include src="assets/deconv1d.html"><figure id="deconv1d" class="w-screen">
</figure>

<script>
deconv1d();
</script>
</dt-include>

  <p>The overlap pattern also forms in two dimensions.
  The uneven overlaps on the two axes multiply together,
  creating a characteristic checkerboard-like pattern of varying magnitudes.</p>

  <dt-include src="assets/deconv2d.html"><figure id="deconv2d" class="w-page">
</figure>

<script>
deconv2d();
</script>
</dt-include>

  <p>In fact, the uneven overlap tends to be more extreme in two dimensions!
  Because the two patterns are multiplied together, the unevenness gets squared.
  For example, in one dimension, a stride 2, size 3 deconvolution has some outputs with twice the number of inputs as others,
  but in two dimensions this becomes a factor of four.</p>

  <p>Now, neural nets typically use multiple layers of deconvolution when creating images,
  iteratively building a larger image out of a series of lower resolution descriptions.
  While it’s possible for these stacked deconvolutions to cancel out artifacts,
  they often compound, creating artifacts on a variety of scales.</p>

  <dt-include src="assets/deconv1d_multi.html"><figure id="deconv1d_multi" class="w-screen">
</figure>

<script>
deconv1d_multi();
</script>
</dt-include>

  <p>Stride 1 deconvolutions —
  which we often see as the last layer in successful models (eg. <dt-cite key="salimans2016improved"><span id="citation-6" data-hover-ref="dt-cite-hover-box-6"><span class="citation-number">[2]</span></span></dt-cite>)
  — are quite effective at dampening artifacts.
   They can remove artifacts of frequencies
  that divide their size, and reduce others artifacts of frequency less than their
  size. However, artifacts can still leak through, as seen in many recent models.</p>

  <p>In addition to the high frequency checkerboard-like artifacts we observed above,
  early deconvolutions can create lower-frequency artifacts,
  which we’ll explore in more detail later.</p>

  <p>These artifacts tend to be most prominent when outputting unusual colors.
  Since neural network layers typically have a bias
  (a learned value added to the output) it’s easy to output the average color.
  The further a color — like bright red — is away from the average color,
  the more deconvolution needs to contribute.</p>

  <hr>

  <h2>Overlap &amp; Learning</h2>

  <p>Thinking about things in terms of uneven overlap is — while a useful framing —
  kind of simplistic. For better or worse, our models learn weights for their deconvolutions.</p>

  <p>In theory, our models could learn to carefully write to unevenly overlapping positions so that the output
  is evenly balanced.</p>

  <figure class="l-page">
  <img src="assets/upsample_LearnedConvUneven.svg">
  </figure>

  <p>This is a tricky balancing act to achieve, especially when one has multiple channels interacting.
  Avoiding artifacts significantly restricts the possible filters, sacrificing model capacity.
  In practice, neural networks struggle to learn to completely avoid these patterns.</p>

  <p>In fact, not only do models with uneven overlap not learn to avoid this,
  but models with even overlap often learn kernels that cause similar artifacts!
  While it isn’t their default behavior the way it is for uneven overlap,
  it’s still very easy for even overlap deconvolution to cause artifacts.</p>

  <figure class="l-page">
    <img src="assets/upsample_LearnedConvEven.svg">
  </figure>

  <p>Completely avoiding artifacts is still a significant restriction on filters,
  and in practice the artifacts are still present in these models, although they seem milder.
  (See <dt-cite key="dumoulin2016adversarially"><span id="citation-7" data-hover-ref="dt-cite-hover-box-7"><span class="citation-number">[4]</span></span></dt-cite>,
  which uses stride 2 size 4 deconvolutions, as an example.)</p>

  <p>There are probably a lot of factors at play here.
  For example, in the case of Generative Adversarial Networks (GANs), one issue may be the discriminator and its gradients
  (we’ll discuss this more later).
  But a big part of the problem seems to be deconvolution.
  At best, deconvolution is fragile because it very easily represents artifact creating functions, even when the size is carefully chosen.
  At worst, creating artifacts is the default behavior of deconvolution.</p>

  <p>Is there a different way to upsample that is more resistant to artifacts?</p>

  <hr>

  <h2>Better Upsampling</h2>

  <p>To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”).
  Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior.
  Ideally, it would go further, and be biased against such artifacts.</p>

  <p>One approach is to make sure you use a kernel size that is divided by your stride,
  avoiding the overlap issue.
  This is equivalent to “sub-pixel convolution,” a technique which has recently
  had success in image super-resolution <dt-cite key="shi2016real"><span id="citation-8" data-hover-ref="dt-cite-hover-box-8"><span class="citation-number">[8]</span></span></dt-cite>.
  However, while this approach helps, it is still easy for deconvolution to fall into creating artifacts.</p>

  <p>Another approach is to separate out upsampling to a higher resolution from convolution to compute features.
  For example, you might resize the image (using <a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation">nearest-neighbor interpolation</a> or <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">bilinear interpolation</a>) and then do a convolutional layer.
  This seems like a natural approach, and roughly similar methods have worked well in image super-resolution (eg. <dt-cite key="dong2014image"><span id="citation-9" data-hover-ref="dt-cite-hover-box-9"><span class="citation-number">[9]</span></span></dt-cite>).</p>

  <figure class="l-page">
  <img src="assets/upsample_DeconvTypes.svg">
  </figure>

  <p>Both deconvolution and the different resize-convolution approaches are linear operations, and can be interpreted as matrices.
  This a helpful way to see the differences between them.
  Where deconvolution has a unique entry for each output window, resize-convolution is implicitly weight-tying in a way that discourages high frequency artifacts.</p>

  <p>We’ve had our best results with nearest-neighbor interpolation, and had difficulty making bilinear resize work.
  This may simply mean that, for our models, the nearest-neighbor happened to work well with hyper-parameters optimized for deconvolution.
  It might also point at trickier issues with naively using bilinear interpolation, where it resists high-frequency image features too strongly.
  We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.</p>

  <h3>Code</h3>

  <p>Resize-convolution layers can be easily implemented in TensorFlow using <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/image.html#resize_images"><code>tf.image.resize_images()</code></a>. For best results, use <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#pad"><code>tf.pad()</code></a> before doing convolution with <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d"><code>tf.nn.conv2d()</code></a> to avoid boundary artifacts.</p>

  <hr>

  <h2>Image Generation Results</h2>

  <p>Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts.</p>

  <p>One case where we’ve found this approach to help is Generative Adversarial Networks. Simply switching out the standard deconvolutional layers for nearest-neighbor resize followed by convolution causes artifacts of different frequencies to disappear.</p>

  <dt-include src="assets/deconv_fixes.html"><style>

.fix-examples > .row {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  /*align-items: center;*/
  margin-bottom: 10px;
}

.fix-examples > .row > .example {
  flex-grow: 1;
  width: 100%;
  margin-right: 10px;
}

.fix-examples > .row > .explanation {
  flex-grow: 2;
  width: 200%;
  margin-left: 10px;
}

.fix-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

</style>

<figure class="fix-examples l-body-outset">
  <div class="row">
    <div class="example">
      <img src="assets/fix_two_deconv_boat.png">
    </div>
    <div class="example">
      <img src="assets/fix_two_deconv_deer.png">
    </div>
    <div class="example">
      <img src="assets/fix_two_deconv_prototype_plane.png">
    </div>
    <div class="example">
      <img src="assets/fix_two_deconv_bird.png">
    </div>
    <div class="explanation">
      <figcaption>
        Deconv in last two layers.<br> Other layers use resize-convolution.<br><i>Artifacts of frequency 2 and 4.</i>
      </figcaption>
    </div>
  </div>
  <div class="row">
    <div class="example">
      <img src="assets/fix_one_deconv_boat.png">
    </div>
    <div class="example">
      <img src="assets/fix_one_deconv_deer.png">
    </div>
    <div class="example">
      <img src="assets/fix_one_deconv_prototype_plane.png">
    </div>
    <div class="example">
      <img src="assets/fix_one_deconv_bird.png">
    </div>
    <div class="explanation">
      <figcaption>
        Deconv only in last layer.<br> Other layers use resize-convolution.<br><i>Artifacts of frequency 2.</i>
      </figcaption>
    </div>
  </div>
  <div class="row">
    <div class="example">
      <img src="assets/fix_zero_deconv_boat.png">
    </div>
    <div class="example">
      <img src="assets/fix_zero_deconv_deer.png">
    </div>
    <div class="example">
      <img src="assets/fix_zero_deconv_prototype_plane.png">
    </div>
    <div class="example">
      <img src="assets/fix_zero_deconv_bird.png">
    </div>
    <div class="explanation">
      <figcaption>
        All layers use resize-convolution.<br><i>No artifacts.</i>
      </figcaption>
    </div>
  </div>
</figure>
</dt-include>

  <p>In fact, the difference in artifacts can be seen before any training occurs.
  If we look at the images the generator produces, initialized with random weights,
  we can already see the artifacts:</p>

  <dt-include src="assets/deconv_fixes_step0.html"><style>

.fix-step0-examples > .row {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  /*align-items: center;*/
  margin-bottom: 10px;
}

.fix-step0-examples > .row > .exampleA {
  flex-grow: 1;
  width: 100%;
  margin-right: 10px;
}
.fix-step0-examples > .row > .exampleB {
  flex-grow: 1;
  width: 100%;
  margin-right: 20px;
}

.fix-step0-examples > .row > .explanation {
  flex-grow: 2;
  width: 200%;
}

.fix-step0-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

</style>

<figure class="fix-step0-examples l-body-outset">
  <div class="row">
    <div class="exampleA">
      <img src="assets/im_rand_two_0.png">
    </div>
    <div class="exampleB">
      <img src="assets/im_rand_two_1.png">
    </div>
    <div class="exampleA">
      <img src="assets/im_rand_one_0.png">
    </div>
    <div class="exampleB">
      <img src="assets/im_rand_one_1.png">
    </div>
    <div class="exampleA">
      <img src="assets/im_rand_zero_1.png">
    </div>
    <div class="exampleB">
      <img src="assets/im_rand_zero_2.png">
    </div>
  </div>
  <div class="row">
    <div class="explanation">
      <figcaption>
        Deconvolution in last two layers.<br> <i>Artifacts prior to any training.</i>
      </figcaption>
    </div>
    <div class="explanation">
      <figcaption>
        Deconvolution only in last layer.<br><i>Artifacts prior to any training.</i>
      </figcaption>
    </div>
    <div class="explanation">
      <figcaption>
        All layers use resize-convolution.<br><i>No artifacts before or after training.</i>
      </figcaption>
    </div>
  </div>

</figure>
</dt-include>

  <p>This suggests that the artifacts are due to this method of generating images, rather than adversarial training.
  (It also suggests that we might be able to learn a lot about good generator design without the slow feedback cycle of training models.)</p>

  <p>Another reason to believe these artifacts aren’t GAN specific is that we see them in other kinds of models, and have found that they also go away when we switch to resize-convolution upsampling.
  For example, consider <dt-cite key="johnson2016perceptual"><span id="citation-10" data-hover-ref="dt-cite-hover-box-10">real-time artistic style transfer <span class="citation-number">[10]</span></span></dt-cite> where a neural net is trained to directly generate style-transferred images.
  We’ve found these to be vulnerable to checkerboard artifacts (especially when the cost doesn’t explicitly resist them).
  However, switching deconvolutional layers for resize-convolution layers makes the artifacts disappear.</p>

  <dt-include src="assets/style_fix.html"><style>

.style-examples > .row {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  /*align-items: center;*/
  margin-bottom: 10px;
}

.style-examples > .row > .example {
  flex-grow: 4;
  width: 400%;
  margin-right: 10px;
}

.style-examples > .row > .explanation {
  flex-grow: 2;
  width: 200%;
  margin-left: 10px;
}

.style-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

</style>

<figure class="style-examples l-body-outset">
  <div class="row">
    <div class="example">
      <img src="assets/style_artifacts.png">
    </div>
    <div class="explanation">
      <figcaption>
        Using deconvolution.<br><i>Heavy checkerboard artifacts.</i>
      </figcaption>
    </div>
  </div>
  <div class="row">
    <div class="example">
      <img src="assets/style_clean.png">
    </div>
    <div class="explanation">
      <figcaption>
        Using resize-convolution.<br><i>No checkerboard artifacts.</i>
      </figcaption>
    </div>
  </div>
</figure>
</dt-include>

  <p>Forthcoming papers from the Google Brain team will demonstrate the benefits of this technique
  in more thorough experiments and state-of-the-art results.
  (We’ve chosen to present this technique separately because we felt it merited more detailed discussion, and because it cut across multiple papers.)</p>

  <hr>

  <h2>Artifacts in Gradients</h2>

  <p>Whenever we compute the gradients of a convolutional layer,
  we do deconvolution (transposed convolution) on the backward pass.
  This can cause checkerboard patterns in the gradient,
  just like when we use deconvolution to generate images.</p>

  <p>The presence of high-frequency “noise” in image model gradients is
  already known in the feature visualization community, where it’s a major challenge.
  Somehow, feature visualization methods must compensate for this noise.</p>

  <p>For example, DeepDream <dt-cite key="mordvintsev2015inceptionism"><span id="citation-11" data-hover-ref="dt-cite-hover-box-11"><span class="citation-number">[11]</span></span></dt-cite>
  seems to cause destructive interference between artifacts in a number of ways,
  such as optimizing many features simultaneously, and optimizing at many offsets and scales.
  In particular, the “jitter” of optimizing at different offsets cancels out some of the checkerboard artifacts.</p>

  <dt-include src="assets/deepdream_fix.html"><style>
.deepdream-examples {
  position: relative;
}

.deepdream-examples::after {
  overflow: visible;
  content: "";
  background-image: url("assets/pointer.svg");
  width: 27px;
  height: 27px;
  position: absolute;
  right: -51px;
  top: 0;
}

.deepdream-examples .example {
  position: relative;
}

.deepdream-examples .original {
  position: relative;
  width: 45%;
}

.deepdream-examples .original:after {
  content: "";
  display: block;
  width: 100%;
  height: 70%;
  position: absolute;
  top: -20%;
  z-index: 1;
}

.deepdream-examples .example .reticle {
  position: absolute;
  pointer-events: none;
}

.deepdream-examples .example .reticle:after {
  box-sizing: border-box;
  position: relative;
  width: 100%;
  height: 100%;
  left: -50%;
  top: -50%;
  border-radius: 50%;
  border: 1px solid white;
  display: block;
  content: '';
  background-color: rgba(255, 255, 255, 0.2);
  box-shadow: 0 0 4px rgba(0, 0, 0, 0.5);
}

.deepdream-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

.deepdream-examples .closeup img {
  -webkit-filter: contrast(150%);
  filter: contrast(150%);
}

.deepdream-examples .closeup {
  position: absolute;
  right: 34%;
  top: 0;
  width: 22.5%;
  border-radius: 50%;
  margin-bottom: 12px;
  border: 2px solid white;
  overflow: hidden;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
}

.deepdream-examples figcaption {
  position: absolute;
  right: 0%;
  top: 0;
  width: 30%;
}

.deepdream-examples .closeup img {
  position: absolute;
  width: 800%;
}

.deepdream-examples .closeup:after {
  padding-top: 100%;
  display: block;
  content: '';
}

</style>

<div class="deepdream-examples l-body-outset">
    <figure class="example" style="margin-bottom:20px;">
      <div class="original" data-focus="10,0.2,0.73">
        <img src="assets/deepdream_full_gitter_0x0.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/deepdream_full_gitter_0x0.png">
      </div>
      <figcaption>
        DeepDream only applying the neural network to a fixed position.<br>
        <i>Severe artifacts.</i>
      </figcaption>
    </figure>
    <figure class="example" style="margin-top:20px;">
      <div class="original" data-focus="10,0.035,0.74">
        <img src="assets/deepdream_full_gitter_8x8.png">
        <div class="reticle"></div>
      </div>
      <div class="closeup">
        <img src="assets/deepdream_full_gitter_8x8.png">
      </div>
      <figcaption>
        DeepDream applying the network to a different position each step.<br>
        <i>Reduced artifacts.</i>
      </figcaption>
    </figure>
</div>

<script>(function() {

var html = d3.select(".deepdream-examples");
var original = html.selectAll(".example .original");
html
    .on("mouseleave", () => {
      console.log("out")
      original.each(resetReticle);
    });

original
    .datum(function() {
      var focus = this.getAttribute("data-focus").split(",")
      return {
        zoom: focus[0],
        x: focus[1],
        y: focus[2]
      };
    })
    .on("mouseleave", resetReticle)
    .on("mousemove", updateReticle)
    .each(resetReticle);

function updateReticle(d) {

  original.each(resetReticle);
  var x = d3.event.offsetX / this.getBoundingClientRect().width;
  var y = d3.event.offsetY / this.getBoundingClientRect().height;
  setPosition(this, x, y, d.zoom, 100);
}

function resetReticle(d) {
  setPosition(this, d.x, d.y, d.zoom, 300);
}

function setPosition(element, x, y, zoom, duration) {
  var marginX = 1 / zoom / 4;
  var marginY = 1 / zoom / 2;
  x = Math.min(Math.max(marginX, x), 1 - marginX)
  y = Math.min(Math.max(marginY, y), 1 - marginY)
  var parent = d3.select(element.parentElement);
  var reticle = parent.select(".reticle");
  var closeup = parent.select(".closeup img");

  reticle
      .style("width", 50 / zoom + "%")
      .style("height", 100 / zoom + "%")
      .transition()
      .ease(d3.ease("cubic-out"))
      .duration(duration)
      .styleTween("left", function (d, i, a) {
        var from = this.style.left,
            to = x * 100 + "%";
        return d3.interpolateString(from, to);
      })
      .styleTween("top", function (d, i, a) {
        var from = this.style.top,
            to = y * 100 + "%";
        return d3.interpolateString(from, to);
      })

  closeup
      .style("width", zoom * 200 + "%")
      .style("height", zoom * 100 + "%")
      .transition()
      .ease(d3.ease("cubic-out"))
      .duration(duration)
      .styleTween("left", function (d, i, a) {
        var from = this.style.left,
            to = -x * 200 * zoom + 50 + "%";
        return d3.interpolateString(from, to);
      })
      .styleTween("top", function (d, i, a) {
        var from = this.style.top,
            to = -y * 100 * zoom + 50 + "%";
        return d3.interpolateString(from, to);
      })
}


})();</script>
</dt-include>

  <p>(While some of the artifacts are our standard checkerboard pattern,
  others are a less organized high-frequency pattern.
  We believe these to be caused by max pooling.
  Max pooling was previously linked to high-frequency artifacts in <dt-cite key="henaff2015geodesics"><span id="citation-12" data-hover-ref="dt-cite-hover-box-12"><span class="citation-number">[12]</span></span></dt-cite>.)</p>

  <p>More recent work in feature visualization (eg. <dt-cite key="mordvintsev2016deepdreaming"><span id="citation-13" data-hover-ref="dt-cite-hover-box-13"><span class="citation-number">[13]</span></span></dt-cite>),
  has explicitly recognized and compensated for these high-frequency gradient components.
  One wonders if better neural network architectures could make these efforts unnecessary.</p>

  <p>Do these gradient artifacts affect GANs?
  If gradient artifacts can affect an image being optimized based on a neural networks gradients in feature visualization,
  we might also expect it to affect the family of images parameterized by the generator as they’re optimized by the discriminator in GANs.</p>

  <p>We’ve found that this does happen in some cases.
  When the generator is neither biased for or against checkerboard patterns,
  strided convolutions in the discriminator can cause them.</p>

  <dt-include src="assets/discrim_fixes.html"><style>

.discrim-examples > .row {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  /*align-items: center;*/
  margin-bottom: 10px;
}

.discrim-examples > .row > .exampleA {
  flex-grow: 1;
  width: 100%;
  margin-right: 10px;
}
.discrim-examples > .row > .exampleB {
  flex-grow: 1;
  width: 100%;
  margin-right: 20px;
}

.discrim-examples > .row > .explanationA {
  flex-grow: 3;
  width: 300%;
  padding-right: 20px;
}

.discrim-examples > .row > .explanationB {
  flex-grow: 3;
  width: 300%;
  padding-right: 10px;
}

.discrim-examples img {
  display: block;
  width: 100%;
  -ms-interpolation-mode: nearest-neighbor;
  image-rendering: optimizeSpeed;
  image-rendering: pixelated;
}

</style>

<figure class="discrim-examples l-body-outset">
  <div class="row">
    <div class="exampleA">
      <img src="assets/discrim_stride2_0.png">
    </div>
    <div class="exampleA">
      <img src="assets/discrim_stride2_1.png">
    </div>
    <div class="exampleB">
      <img src="assets/discrim_stride2_2.png">
    </div>
    <div class="exampleA">
      <img src="assets/discrim_stride1_1.png">
    </div>
    <div class="exampleA">
      <img src="assets/discrim_stride1_2.png">
    </div>
    <div class="exampleA">
      <img src="assets/discrim_stride1_0.png">
    </div>
  </div>
  <div class="row">
    <div class="explanationA">
      <figcaption>
        Discriminator has stride 2 convolution in first layer.<br> <i>Strong frequency 2 artifacts.</i>
      </figcaption>
    </div>
    <div class="explanationB">
      <figcaption>
        Discriminator has regular convolution in first layer.<br> <i>Very mild artifacts.</i>
      </figcaption>
    </div>

  </div>

</figure>
</dt-include>

  <p>It’s unclear what the broader implications of these gradient artifacts are.
  One way to think about them is that some neurons will get many times the gradient of their neighbors, basically arbitrarily.
  Equivalently, the network will care much more about some pixels in the input than others, for no good reason.
  Neither of those sounds ideal.</p>

  <p>It seems possible that having some pixels affect the network output much more than others may exaggerate adversarial counter-examples.
  Because the derivative is concentrated on small number of pixels,
  small perturbations of those pixels may have outsized effects.
  We have not investigated this.</p>

  <hr>

  <h2>Conclusion</h2>

  <p>The standard approach of producing images with deconvolution — despite its successes! — has some conceptually simple issues that lead to artifacts in produced images.
  Using a natural alternative without these issues causes the artifacts to go away
  (Analogous arguments suggest that standard strided convolutional layers may also have issues).</p>

  <p>This seems like an exciting opportunity to us!
  It suggests that there is low-hanging fruit to be found in carefully thinking through neural network architectures, even ones where we seem to have clean working solutions.</p>

  <p>In the meantime, we’ve provided an easy to use solution that improves the quality of many approaches to generating images with neural networks. We look forward to seeing what people do with it, and whether it helps in domains like audio, where high frequency artifacts would be particularly problematic.</p>


</dt-article>

<dt-appendix class="centered">
<style>
  dt-appendix {
    display: block;
    font-size: 14px;
    line-height: 24px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0,0,0,0.1);
    color: rgba(0,0,0,0.5);
    background: rgb(250, 250, 250);
    padding-top: 36px;
    padding-bottom: 60px;
  }
  dt-appendix h3 {
    font-size: 16px;
    font-weight: 500;
    margin-top: 18px;
    margin-bottom: 18px;
    color: rgba(0,0,0,0.65);
  }
  dt-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
  }
  dt-appendix .references {
    font-size: 12px;
    line-height: 20px;
  }
  dt-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }
  dt-appendix ol,
  dt-appendix ul {
    padding-left: 24px;
  }
</style>

<div class="l-body">

  <h3>Acknowledgments</h3>

  <p>We are very grateful to Shan Carter for his wonderful improvements to the first interactive diagram, design advice, and editorial taste.
  We’re also very grateful to David Dohan for providing us with an example of strided convolutions in discriminators causing artifacts
  and to Mike Tyka who originally pointed out the connection between jitter and artifacts in DeepDream to us.</p>

  <p>Thank you also to Luke Vilnis, Jon Shlens, Luke Metz, Alex Mordvintsev, and Ben Poole for their feedback and encouragement.</p>

  <p>This work was made possible by the support of the <a href="https://research.google.com/teams/brain/">Google Brain</a> team. Augustus Odena’s work was done as part of the <a href="https://research.google.com/teams/brain/residency/">Google Brain Residency Program</a>. Vincent Dumoulin did this while visiting the Brain Team as an intern.</p>

  <h3>Author Contributions</h3>

  <p>Augustus and Chris recognized the connection between deconvolution and artifacts. Augustus ran the GAN experiments. Vincent ran the artistic style transfer experiments. Chris ran the DeepDream experiments, created the visualizations and wrote most of the article.</p>


<h3>References</h3><dt-bibliography><ol><li><b>Unsupervised representation learning with deep convolutional generative adversarial networks</b>   <a href="https://arxiv.org/pdf/1511.06434.pdf">[PDF]</a><br>Radford, A., Metz, L. and Chintala, S., 2015. arXiv preprint arXiv:1511.06434. </li><li><b>Improved techniques for training gans</b>   <a href="https://arxiv.org/pdf/1606.03498.pdf">[PDF]</a><br>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X., 2016. Advances in Neural Information Processing Systems, pp. 2226—2234. </li><li><b>Adversarial Feature Learning</b>   <a href="https://arxiv.org/pdf/1605.09782.pdf">[PDF]</a><br>Donahue, J., Krahenbuhl, P. and Darrell, T., 2016. arXiv preprint arXiv:1605.09782. </li><li><b>Adversarially Learned Inference</b>   <a href="https://arxiv.org/pdf/1606.00704.pdf">[PDF]</a><br>Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O. and Courville, A., 2016. arXiv preprint arXiv:1606.00704. </li><li><b>A guide to convolution arithmetic for deep learning</b>   <a href="https://arxiv.org/pdf/1603.07285.pdf">[PDF]</a><br>Dumoulin, V. and Visin, F., 2016. arXiv preprint arXiv:1603.07285. </li><li><b>Is the deconvolution layer the same as a convolutional layer?</b>   <a href="https://arxiv.org/pdf/1609.07009.pdf">[PDF]</a><br>Shi, W., Caballero, J., Theis, L., Huszar, F., Aitken, A., Ledig, C. and Wang, Z., 2016. arXiv preprint arXiv:1609.07009. </li><li><b>Conditional generative adversarial nets for convolutional face generation</b>   <a href="http://www.foldl.me/uploads/papers/tr-cgans.pdf">[PDF]</a><br>Gauthier, J., 2014. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, Vol 2014. </li><li><b>Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</b>   <a href="https://arxiv.org/pdf/1609.05158.pdf">[PDF]</a><br>Shi, W., Caballero, J., Huszar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert, D. and Wang, Z., 2016. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1874—1883.  <a href="https://doi.org/10.1109/cvpr.2016.207" style="text-decoration:inherit;">DOI: 10.1109/cvpr.2016.207</a></li><li><b>Image super-resolution using deep convolutional networks</b>   <a href="https://arxiv.org/pdf/1501.00092.pdf">[PDF]</a><br>Dong, C., Loy, C.C., He, K. and Tang, X., 2014. arXiv preprint arXiv:1501.00092. </li><li><b>Perceptual losses for real-time style transfer and super-resolution</b>   <a href="https://arxiv.org/pdf/1603.08155.pdf">[PDF]</a><br>Johnson, J., Alahi, A. and Fei-Fei, L., 2016. arXiv preprint arXiv:1603.08155. </li><li><b>Inceptionism: Going deeper into neural networks</b>   <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">[HTML]</a><br>Mordvintsev, A., Olah, C. and Tyka, M., 2015. Google Research Blog. Retrieved June, Vol 20. </li><li><b>Geodesics of learned representations</b>   <a href="https://arxiv.org/pdf/1511.06394.pdf">[PDF]</a><br>Henaff, O.J. and Simoncelli, E.P., 2015. arXiv preprint arXiv:1511.06394. </li><li><b>DeepDreaming with TensorFlow</b>   <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb">[link]</a><br>Mordvintsev, A., 2016. </li></ol></dt-bibliography><h3>Updates and Corrections</h3>
    <p><a href="https://github.com/distillpub/post--deconv-checkerboard/compare/ee0c32dc45f999dd8b32028bcfe2887560852861...6fad5c3b00269553aef0c7978be57b5635f4486a">View all changes</a> to this article since it was first published. If you see a mistake or want to suggest a change, please <a class="github-issue" href="https://github.com/distillpub/post--deconv-checkerboard/issues/new">create an issue on GitHub</a>.</p><h3 id="citation">Citations and Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY 2.0</a>, unless noted otherwise, with the <a class="github" href="https://github.com/distillpub/post--deconv-checkerboard">source available on GitHub</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: “Figure from …”.</p>

    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Odena, et al., "Deconvolution and Checkerboard Artifacts", Distill, 2016. http://doi.org/10.23915/distill.00003</pre>

    <p>BibTeX citation</p>
<pre class="citation long">@article{odena2016deconvolution,
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  title = {Deconvolution and Checkerboard Artifacts},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/deconv-checkerboard},
  doi = {10.23915/distill.00003}
}</pre></div>
</dt-appendix>

<script type="text/bibliography">
@article{dong2014image,
  title={Image super-resolution using deep convolutional networks},
  author={Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  journal={arXiv preprint arXiv:1501.00092},
  year={2014},
  url={https://arxiv.org/pdf/1501.00092.pdf}
}

@article{dumoulin2016adversarially,
  title={Adversarially Learned Inference},
  author={Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Lamb, Alex and Arjovsky, Martin and Mastropietro, Olivier and Courville, Aaron},
  journal={arXiv preprint arXiv:1606.00704},
  year={2016},
  url={https://arxiv.org/pdf/1606.00704.pdf}
}

@article{dumoulin2016guide,
  title={A guide to convolution arithmetic for deep learning},
  author={Dumoulin, Vincent and Visin, Francesco},
  journal={arXiv preprint arXiv:1603.07285},
  year={2016},
  url={https://arxiv.org/pdf/1603.07285.pdf}
}

@article{donahue2016adversarial,
  title={Adversarial Feature Learning},
  author={Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  journal={arXiv preprint arXiv:1605.09782},
  year={2016},
  url={https://arxiv.org/pdf/1605.09782.pdf}
}

@article{gauthier2014conditional,
  title={Conditional generative adversarial nets for convolutional face generation},
  author={Gauthier, Jon},
  journal={Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester},
  volume={2014},
  year={2014},
  url={http://www.foldl.me/uploads/papers/tr-cgans.pdf}
}

@article{henaff2015geodesics,
  title={Geodesics of learned representations},
  author={H{\'e}naff, Olivier J and Simoncelli, Eero P},
  journal={arXiv preprint arXiv:1511.06394},
  year={2015},
  url={https://arxiv.org/pdf/1511.06394.pdf}
}

@article{johnson2016perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1603.08155},
  year={2016},
  url={https://arxiv.org/pdf/1603.08155.pdf}
}

@article{mordvintsev2015inceptionism,
  title={Inceptionism: Going deeper into neural networks},
  author={Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
  journal={Google Research Blog. Retrieved June},
  volume={20},
  year={2015},
  url={https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}

@misc{mordvintsev2016deepdreaming,
  title={DeepDreaming with TensorFlow},
  author={Mordvintsev, Alexander},
  year={2016},
  url={https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb},
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015},
  url={https://arxiv.org/pdf/1511.06434.pdf}
}

@inproceedings{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2226--2234},
  year={2016},
  url={https://arxiv.org/pdf/1606.03498.pdf}
}

@article{shi2016deconvolution,
  title={Is the deconvolution layer the same as a convolutional layer?},
  author={Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Ledig, Christian and Wang, Zehan},
  journal={arXiv preprint arXiv:1609.07009},
  year={2016},
  url={https://arxiv.org/pdf/1609.07009.pdf}
}

@inproceedings{shi2016real,
  title={Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network},
  author={Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1874--1883},
  year={2016},
  url={https://arxiv.org/pdf/1609.05158.pdf},
  doi={10.1109/cvpr.2016.207}
}

</script>

<style>
    dt-cite {
      color: hsla(206, 90%, 20%, 0.7);
    }
    dt-cite .citation-number {
      cursor: default;
      white-space: nowrap;
      font-family: -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
      font-size: 75%;
      color: hsla(206, 90%, 20%, 0.7);
      display: inline-block;
      line-height: 1.1em;
      text-align: center;
      position: relative;
      top: -2px;
      margin: 0 2px;
    }
    figcaption dt-cite .citation-number {
      font-size: 11px;
      font-weight: normal;
      top: -2px;
      line-height: 1em;
    }
  </style><div id="cite-hover-boxes-container"><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-0"><b>Unsupervised representation learning with deep convolutional generative adversarial networks</b>  <a href="https://arxiv.org/pdf/1511.06434.pdf">[PDF]</a><br>A. Radford, L. Metz, S. Chintala.<br>arXiv preprint arXiv:1511.06434. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-1"><b>Improved techniques for training gans</b>  <a href="https://arxiv.org/pdf/1606.03498.pdf">[PDF]</a><br>T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen.<br>Advances in Neural Information Processing Systems, pp. 2226—2234. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-2"><b>Adversarial Feature Learning</b>  <a href="https://arxiv.org/pdf/1605.09782.pdf">[PDF]</a><br>J. Donahue, P. Krahenbuhl, T. Darrell.<br>arXiv preprint arXiv:1605.09782. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-3"><b>Adversarially Learned Inference</b>  <a href="https://arxiv.org/pdf/1606.00704.pdf">[PDF]</a><br>V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, A. Courville.<br>arXiv preprint arXiv:1606.00704. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-4"><b>A guide to convolution arithmetic for deep learning</b>  <a href="https://arxiv.org/pdf/1603.07285.pdf">[PDF]</a><br>V. Dumoulin, F. Visin.<br>arXiv preprint arXiv:1603.07285. 2016. <br><br><b>Is the deconvolution layer the same as a convolutional layer?</b>  <a href="https://arxiv.org/pdf/1609.07009.pdf">[PDF]</a><br>W. Shi, J. Caballero, L. Theis, F. Huszar, A. Aitken, C. Ledig, Z. Wang.<br>arXiv preprint arXiv:1609.07009. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-5"><b>Conditional generative adversarial nets for convolutional face generation</b>  <a href="http://www.foldl.me/uploads/papers/tr-cgans.pdf">[PDF]</a><br>J. Gauthier.<br>Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, Vol 2014. 2014. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-6"><b>Improved techniques for training gans</b>  <a href="https://arxiv.org/pdf/1606.03498.pdf">[PDF]</a><br>T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen.<br>Advances in Neural Information Processing Systems, pp. 2226—2234. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-7"><b>Adversarially Learned Inference</b>  <a href="https://arxiv.org/pdf/1606.00704.pdf">[PDF]</a><br>V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, A. Courville.<br>arXiv preprint arXiv:1606.00704. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-8"><b>Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</b>  <a href="https://arxiv.org/pdf/1609.05158.pdf">[PDF]</a><br>W. Shi, J. Caballero, F. Huszar, J. Totz, A.P. Aitken, R. Bishop, D. Rueckert, Z. Wang.<br>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1874—1883. 2016. <br> <a href="https://doi.org/10.1109/cvpr.2016.207" style="text-decoration:inherit;">DOI: 10.1109/cvpr.2016.207</a></div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-9"><b>Image super-resolution using deep convolutional networks</b>  <a href="https://arxiv.org/pdf/1501.00092.pdf">[PDF]</a><br>C. Dong, C.C. Loy, K. He, X. Tang.<br>arXiv preprint arXiv:1501.00092. 2014. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-10"><b>Perceptual losses for real-time style transfer and super-resolution</b>  <a href="https://arxiv.org/pdf/1603.08155.pdf">[PDF]</a><br>J. Johnson, A. Alahi, L. Fei-Fei.<br>arXiv preprint arXiv:1603.08155. 2016. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-11"><b>Inceptionism: Going deeper into neural networks</b>  <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">[HTML]</a><br>A. Mordvintsev, C. Olah, M. Tyka.<br>Google Research Blog. Retrieved June, Vol 20. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-12"><b>Geodesics of learned representations</b>  <a href="https://arxiv.org/pdf/1511.06394.pdf">[PDF]</a><br>O.J. Henaff, E.P. Simoncelli.<br>arXiv preprint arXiv:1511.06394. 2015. </div><div style="display:none;" class="dt-hover-box" id="dt-cite-hover-box-13"><b>DeepDreaming with TensorFlow</b>  <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb">[link]</a><br>A. Mordvintsev.  2016. </div></div><div id="footnote-hover-boxes-container"></div><script>

function nodeFromString(str) {
  var div = document.createElement("div");
  div.innerHTML = str;
  return div.firstChild;
}

function make_hover_css(pos) {
  var pretty = window.innerWidth > 600;
  var padding = pretty? 18 : 12;
  var outer_padding = pretty ? 18 : 0;
  var bbox = document.querySelector("body").getBoundingClientRect();
  var left = pos[0] - bbox.left, top = pos[1] - bbox.top;
  var width = Math.min(window.innerWidth-2*outer_padding, 648);
  left = Math.min(left, window.innerWidth-width-outer_padding);
  width = width - 2*padding;
  return (`position: absolute;
     background-color: #FFF;
     opacity: 0.95;
     max-width: ${width}px;
     top: ${top}px;
     left: ${left}px;
     border: 1px solid rgba(0, 0, 0, 0.25);
     padding: ${padding}px;
     border-radius: ${pretty? 3 : 0}px;
     box-shadow: 0px 2px 10px 2px rgba(0, 0, 0, 0.2);
     z-index: ${1e6};`);
}


function DtHoverBox(div_id) {
  this.div = document.querySelector("#"+div_id);
  this.visible = false;
  this.bindDivEvents();
  DtHoverBox.box_map[div_id] = this;
}

DtHoverBox.box_map = {};

DtHoverBox.get_box = function get_box(div_id) {
  if (div_id in DtHoverBox.box_map) {
    return DtHoverBox.box_map[div_id];
  } else {
    return new DtHoverBox(div_id);
  }
}

DtHoverBox.prototype.show = function show(pos){
  this.visible = true;
  this.div.setAttribute("style", make_hover_css(pos) );
  for (var box_id in DtHoverBox.box_map) {
    var box = DtHoverBox.box_map[box_id];
    if (box != this) box.hide();
  }
}

DtHoverBox.prototype.showAtNode = function showAtNode(node){
    var bbox = node.getBoundingClientRect();
    this.show([bbox.right, bbox.bottom]);
}

DtHoverBox.prototype.hide = function hide(){
  this.visible = false;
  if (this.div) this.div.setAttribute("style", "display:none");
  if (this.timeout) clearTimeout(this.timeout);
}

DtHoverBox.prototype.stopTimeout = function stopTimeout() {
  if (this.timeout) clearTimeout(this.timeout);
}

DtHoverBox.prototype.extendTimeout = function extendTimeout(T) {
  //console.log("extend", T)
  var this_ = this;
  this.stopTimeout();
  this.timeout = setTimeout(function(){this_.hide();}.bind(this), T);
}

// Bind events to a link to open this box
DtHoverBox.prototype.bind = function bind(node) {
  if (typeof node == "string"){
    node = document.querySelector(node);
  }

  node.addEventListener("mouseover", function(){
    if (!this.visible) this.showAtNode(node);
    this.stopTimeout();
  }.bind(this));

  node.addEventListener("mouseout", function(){this.extendTimeout(250);}.bind(this));

  node.addEventListener("touchstart", function(e) {
    if (this.visible) {
      this.hide();
    } else {
      this.showAtNode(node);
    }
    // Don't trigger body touchstart event when touching link
    e.stopPropagation();
  }.bind(this));
}

DtHoverBox.prototype.bindDivEvents = function bindDivEvents(){
  // For mice, same behavior as hovering on links
  this.div.addEventListener("mouseover", function(){
    if (!this.visible) this.showAtNode(node);
    this.stopTimeout();
  }.bind(this));
  this.div.addEventListener("mouseout", function(){this.extendTimeout(250);}.bind(this));

  // Don't trigger body touchstart event when touching within box
  this.div.addEventListener("touchstart", function(e){e.stopPropagation();});
  // Close box when touching outside box
  document.body.addEventListener("touchstart", function(){this.hide();}.bind(this));
}

var hover_es = document.querySelectorAll("span[data-hover-ref]");
hover_es = [].slice.apply(hover_es);
hover_es.forEach(function(e,n){
  var ref_id = e.getAttribute("data-hover-ref");
  DtHoverBox.get_box(ref_id).bind(e);
})
</script><dt-footer>
<style>
dt-footer {
  display: block;
  color: rgba(255, 255, 255, 0.4);
  font-weight: 300;
  padding: 40px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(200, 60%, 15%);
  text-align: center;
}
dt-footer .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
dt-footer .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
dt-footer .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}
dt-footer .nav {
  margin-top: 12px;
}
dt-footer .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
}
</style>

<div class="l-page">
  <div class="description">
  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  is dedicated to clear explanations of machine learning
  </div>
  <div class="nav">
    <a href="http://distill.pub/about/">About</a>
    <a href="http://distill.pub/journal/">Submit</a>
    <a href="http://distill.pub/prize/">Prize</a>
    <a href="http://distill.pub/archive/">Archive</a>
    <a href="http://distill.pub/rss.xml">RSS</a>
    <a href="https://github.com/distillpub">GitHub</a>
    <a href="https://twitter.com/distillpub">Twitter</a>
    &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
  </div>
</div>
</dt-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body></html>
